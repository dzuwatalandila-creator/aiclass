{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dzuwatalandila-creator/aiclass/blob/main/Thursday.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jTdT7lLcMD5F",
        "outputId": "7eaa5952-9742-4b1f-d3a4-3cfae4c368c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.76)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.24)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.24.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (0.3.33)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.76 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.3.76)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.104.2 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.106.1)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.11.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain-openai) (0.4.24)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain-openai) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain-openai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain-openai) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain-openai) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain-openai) (25.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain-openai) (2.11.7)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.5)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.104.2->langchain-openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.104.2->langchain-openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.104.2->langchain-openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.104.2->langchain-openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.76->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain-openai) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain-openai) (0.24.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain-openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain-openai) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain\n",
        "!pip install langchain-openai\n",
        "!pip install langchain-community -q\n",
        "!pip install langchain-experimental -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZPOTp8kKjfl"
      },
      "outputs": [],
      "source": [
        "def define_term(term: str) -> str:\n",
        "    \"\"\"Return a one-sentence definition of an AI term.\"\"\"\n",
        "    definitions = {\n",
        "        \"transformer\": \"A Transformer is a deep learning architecture that uses self-attention to process sequences in parallel.\",\n",
        "        \"embedding\": \"An embedding is a numerical vector representation of data (like text or images) that captures semantic meaning.\",\n",
        "        \"rag\": \"Retrieval-Augmented Generation (RAG) is a technique where a model retrieves external documents to improve its responses.\",\n",
        "        \"llm\": \"A Large Language Model (LLM) is a neural network trained on massive text corpora to understand and generate natural language.\"\n",
        "    }\n",
        "    return definitions.get(term.lower(), f\"Sorry, I don't have a definition for '{term}'.\")\n",
        "\n",
        "\n",
        "def summarize_notes(text: str) -> str:\n",
        "    \"\"\"Summarize student notes into 2–3 sentences.\"\"\"\n",
        "    # Simple placeholder (you can replace with an LLM call later)\n",
        "    sentences = text.split(\". \")\n",
        "    if len(sentences) <= 3:\n",
        "        return text\n",
        "    else:\n",
        "        summary = \" \".join(sentences[:3])\n",
        "        return summary.strip() + \" ... (summary truncated)\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "import gradio as gr\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_core.documents import Document\n",
        "from typing import TypedDict, List, Optional, Tuple, Dict\n",
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "from langchain_core.messages import AIMessage, HumanMessage, BaseMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.agents import initialize_agent, Tool\n",
        "\n",
        "def define_term(term: str) -> str:\n",
        "    \"\"\"Return a one-sentence definition of an AI term.\"\"\"\n",
        "    definitions = {\n",
        "        \"transformer\": \"A Transformer is a deep learning architecture that uses self-attention to process sequences in parallel.\",\n",
        "        \"embedding\": \"An embedding is a numerical vector representation of data (like text or images) that captures semantic meaning.\",\n",
        "        \"rag\": \"Retrieval-Augmented Generation (RAG) is a technique where a model retrieves external documents to improve its responses.\",\n",
        "        \"llm\": \"A Large Language Model (LLM) is a neural network trained on massive text corpora to understand and generate natural language.\"\n",
        "    }\n",
        "    return definitions.get(term.lower(), f\"Sorry, I don't have a definition for '{term}'.\")\n",
        "\n",
        "def summarize_notes(text: str) -> str:\n",
        "    \"\"\"Summarize student notes into 2–3 sentences.\"\"\"\n",
        "    sentences = text.split(\". \")\n",
        "    if len(sentences) <= 3:\n",
        "        return text\n",
        "    else:\n",
        "        summary = \" \".join(sentences[:3])\n",
        "        return summary.strip() + \" ... (summary truncated)\"\n",
        "\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"define_term\",\n",
        "        func=define_term,\n",
        "        description=\"Define an AI term in one sentence. Input: term (str).\"\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"summarize_notes\",\n",
        "        func=summarize_notes,\n",
        "        description=\"Summarize student notes into 2–3 sentences. Input: a paragraph of notes (str).\"\n",
        "    )\n",
        "]\n"
      ],
      "metadata": {
        "id": "aOupCFjATLXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hCyXnlo7Z5x2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import userdata\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_community.document_loaders.text import TextLoader\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "from langchain_core.messages import AIMessage, HumanMessage, BaseMessage\n",
        "from typing import TypedDict, List, Optional, Tuple, Dict\n",
        "import gradio as gr\n",
        ""
      ],
      "metadata": {
        "id": "D5bJMlmeYytn"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain_core.tools import tool\n",
        "from langchain.agents import create_tool_calling_agent\n",
        "from langchain.agents import AgentExecutor\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "import time"
      ],
      "metadata": {
        "id": "1jKFRNfMY2p3"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ZhipuAI_embeddings:\n",
        "    def __init__(self, model_name: str = 'embeddings-3'):\n",
        "        self.model_name = model_name\n",
        "        self.base_url = \"https://open.bigmodel.cn/api/paas/v4\"\n",
        "        self.embedding = self._init_model()\n",
        "    def _init_model(self) -> OpenAIEmbeddings:\n",
        "        return OpenAIEmbeddings(\n",
        "            model=self.model_name,\n",
        "            base_url=self.base_url,\n",
        "            api_key=userdata.get(\"BOIYEN\")\n",
        "        )\n",
        "\n",
        "\n",
        "embeddings = ZhipuAI_embeddings().embedding"
      ],
      "metadata": {
        "id": "IhYEMstvY6k9"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = ChatOpenAI(\n",
        "    base_url=\"https://open.bigmodel.cn/api/paas/v4/\",\n",
        "    api_key=userdata.get(\"BOIYEN\"),\n",
        "    model=\"glm-4.5\"\n",
        ")"
      ],
      "metadata": {
        "id": "nyjJJdeFZHGu"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def doc_parsing(file_path) -> list[Document]:\n",
        "    if file_path.endswith(\"pdf\"):\n",
        "        loader = PyPDFLoader(file_path=file_path)\n",
        "    elif file_path.endswith(\".txt\"):\n",
        "        loader = TextLoader(file_path=file_path)\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "    try:\n",
        "        doc = loader.load()\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading document: {e}\")\n",
        "        return []\n",
        "\n",
        "    semantic_splitter = SemanticChunker(\n",
        "        embeddings=embeddings,\n",
        "        breakpoint_threshold_type=\"percentile\",\n",
        "        breakpoint_threshold_amount=95\n",
        "    )\n",
        "    full_text = doc[0].page_content if doc else \"\"\n",
        "    if not full_text:\n",
        "        return []\n",
        "\n",
        "    raw_chunks = semantic_splitter.split_text(full_text)\n",
        "    print(f\"Number of chunks created: {len(raw_chunks)}\")\n",
        "    docs = [Document(page_content=chunk, metadata=doc[0].metadata) for chunk in raw_chunks]\n",
        "    return docs\n",
        "\n",
        "vector_store = InMemoryVectorStore(embeddings)\n",
        ""
      ],
      "metadata": {
        "id": "L9m7yymqZTjl"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tool\n",
        "def define_term(term: str) -> str:\n",
        "    \"\"\"Returns a one-sentence definition of a given AI term. Use this tool when a student asks 'What is X?' or 'define Y'.\"\"\"\n",
        "    definitions = {\n",
        "        \"transformer\": \"A model architecture that uses attention mechanisms to weigh the importance of different parts of the input sequence.\",\n",
        "        \"embedding\": \"A numerical representation of text that captures its semantic meaning for use in machine learning models.\",\n",
        "        \"rag\": \"A technique that retrieves information from an external knowledge base to improve the accuracy and relevance of an LLM's response.\",\n",
        "    }\n",
        "    return definitions.get(term.lower(), f\"I'm sorry, I don't have a definition for '{term}'.\")\n"
      ],
      "metadata": {
        "id": "qQ639VmrZYod"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tool\n",
        "def summarize_notes(text: str) -> str:\n",
        "    \"\"\"Takes a block of student notes (a paragraph) and returns a concise summary of 2-3 sentences. Use this tool when a student provides a long paragraph and asks to summarize it.\"\"\"\n",
        "    summary_chain = ChatPromptTemplate.from_template(\n",
        "        \"Summarize the following text in 2-3 sentences:\\n\\n{text}\"\n",
        "    ) | client\n",
        "    return summary_chain.invoke({\"text\": text}).content\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "D_AWk90BZbJG"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def retrieve_docs(question: str) -> str:\n",
        "    \"\"\"Retrieves relevant documents from the vector store based on a student's question.\"\"\"\n",
        "    if not vector_store.get_all_documents():\n",
        "        return \"No documents have been uploaded to search from.\"\n",
        "\n",
        "    SUB_QUERY_TEMPLATE = \"\"\"\n",
        "    You are a helpful assistant that generates multiple search queries based on a single input query.\n",
        "    Generate 3 diverse search queries related to the user's question, which can be used to retrieve relevant documents.\n",
        "    The queries should be concise and cover different aspects or angles of the original question.\n",
        "\n",
        "    Original Question: {question}\n",
        "\n",
        "    Generated Queries:\n",
        "    -\n",
        "    \"\"\"\n",
        "    sub_query_chain = ChatPromptTemplate.from_template(SUB_QUERY_TEMPLATE) | client\n",
        "    response = sub_query_chain.invoke({\"question\": question})\n",
        "    queries = [q.strip() for q in response.content.split('-') if q.strip()]\n",
        "\n",
        "    all_retrieved_docs = []\n",
        "    seen_doc_contents = set()\n",
        "    for query in queries:\n",
        "        retrieved_for_query = vector_store.similarity_search(query)\n",
        "        for doc in retrieved_for_query:\n",
        "            if doc.page_content not in seen_doc_contents:\n",
        "                all_retrieved_docs.append(doc)\n",
        "                seen_doc_contents.add(doc.page_content)\n",
        "\n",
        "    if not all_retrieved_docs:\n",
        "        return \"No relevant information found in the uploaded documents.\"\n",
        "\n",
        "    context_text = \"\\n\\n\".join(d.page_content for d in all_retrieved_docs)\n",
        "    return context_text"
      ],
      "metadata": {
        "id": "sdJ_4csHZeCm"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rag_tool = tool(retrieve_docs)\n",
        "\n",
        "# List available tools for the agent\n",
        "tools = [define_term, summarize_notes, rag_tool]\n",
        "\n",
        "# Create the agent with its prompt\n",
        "template_with_tools = \"\"\"\n",
        "You are AICLASS assistant, a smart and helpful assistant at takenolab. You have access to specialized tools to help students with their course content.\n",
        "Your primary goal is to be supportive, precise, and direct.\n",
        "\n",
        "You must answer student questions by first determining if you need a tool.\n",
        "- If the question is about an AI term, use the `define_term` tool.\n",
        "- If the request is to summarize text, use the `summarize_notes` tool.\n",
        "- For all other questions about course content (e.g., \"What does the policy say about...?\", \"Explain topic X\"), use the `retrieve_course_content` tool.\n",
        "- If a question is not related to any of these tasks, kindly state that you cannot assist.\n",
        "\n",
        "student problem:\n",
        "{input}\n",
        "\n",
        "{agent_scratchpad}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template_with_tools)\n",
        "llm_with_tools = client.bind_tools(tools)\n",
        "agent = create_tool_calling_agent(llm_with_tools, tools, prompt)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        ""
      ],
      "metadata": {
        "id": "RtfNWxKWZi5u"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_and_answer_with_history(question: str, chat_history: List[Dict[str, str]]) -> List[Dict[str, str]]:\n",
        "    # Convert chat_history (list of dictionaries) to list of BaseMessage for Langchain\n",
        "    history_messages: List[BaseMessage] = []\n",
        "    for message in chat_history:\n",
        "        if message[\"role\"] == \"user\":\n",
        "            history_messages.append(HumanMessage(content=message[\"content\"]))\n",
        "        elif message[\"role\"] == \"assistant\":\n",
        "            history_messages.append(AIMessage(content=message[\"content\"]))\n",
        "\n",
        "    try:\n",
        "        # Invoke the agent with the current question and formatted history\n",
        "        response = agent_executor.invoke({\"input\": question, \"chat_history\": history_messages})\n",
        "        bot_message = response.get(\"output\", \"I'm sorry, I couldn't generate a response.\")\n",
        "\n",
        "        # Append the new interaction to the history in the correct format\n",
        "        chat_history.append({\"role\": \"user\", \"content\": question})\n",
        "        chat_history.append({\"role\": \"assistant\", \"content\": bot_message})\n",
        "\n",
        "        return chat_history\n",
        "\n",
        "    except Exception as e:\n",
        "        error_message = f\"An error occurred: {str(e)}\"\n",
        "        chat_history.append({\"role\": \"user\", \"content\": question})\n",
        "        chat_history.append({\"role\": \"assistant\", \"content\": error_message})\n",
        "        return chat_history\n",
        ""
      ],
      "metadata": {
        "id": "Yn3oZG8ZZmJW"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def doc_loader(file_path):\n",
        "    docs = doc_parsing(file_path)\n",
        "    if not docs:\n",
        "        return \"No content found or processed in the document.\"\n",
        "    _ = vector_store.add_documents(documents=docs)\n",
        "    return f\"Successfully added {len(docs)} document chunks.\"\n",
        ""
      ],
      "metadata": {
        "id": "YTb0Mae4Zo1m"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def interface():\n",
        "    iface = gr.ChatInterface(\n",
        "        fn=retrieve_and_answer_with_history,\n",
        "        chatbot=gr.Chatbot(height=200, type='messages', label=\"Assistant\"),\n",
        "        textbox=gr.Textbox(lines=2,submit_btn=True ),\n",
        "        title=\"Takenolab AIClass Assistant (Conversational RAG)\",\n",
        "        type='messages',\n",
        "        description=\"Ask a question about your course content and get smart advice, supporting multi-turn conversations.\",\n",
        "    )\n",
        "    docs_interface = gr.Interface(\n",
        "        fn=doc_loader,\n",
        "        inputs=gr.File(label=\"Choose a file to upload\",\n",
        "                       type='filepath',\n",
        "                       file_count='single',\n",
        "                       show_label=True\n",
        "                       ),\n",
        "        description=\"Upload a document to run retrieval‐augmented generation.\",\n",
        "        outputs=gr.TextArea()\n",
        "    )\n",
        "    table = gr.TabbedInterface(\n",
        "        [iface,docs_interface],\n",
        "        tab_names= ['Chat', \"Upload File for RAG\"],\n",
        "        title=\"LLM, RAG AND PROMPTS, Text Generation\"\n",
        "    )\n",
        "    iface.launch(debug=True, server_port=3000)\n"
      ],
      "metadata": {
        "id": "gfQy95haZrs2"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interface()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "SNoB--YpZubO",
        "outputId": "24f1e375-9f92-47b5-c8ae-19fb9784b303"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://af5891872a717cac76.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://af5891872a717cac76.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0xfLbLpdvbOpcsiAZCF+N",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}