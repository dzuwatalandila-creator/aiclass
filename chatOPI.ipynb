{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8pXH23RS3pPZcjWSudF93",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dzuwatalandila-creator/aiclass/blob/main/chatOPI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain_openAI"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "h6-rK3zyFduX",
        "outputId": "78bfa57b-d459-41bc-ac3f-d9be23703548"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_openAI\n",
            "  Downloading langchain_openai-0.3.33-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting langchain-core<1.0.0,>=0.3.76 (from langchain_openAI)\n",
            "  Downloading langchain_core-0.3.76-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.104.2 in /usr/local/lib/python3.12/dist-packages (from langchain_openAI) (1.107.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain_openAI) (0.11.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openAI) (0.4.27)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openAI) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openAI) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openAI) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openAI) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openAI) (25.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openAI) (2.11.7)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain_openAI) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain_openAI) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain_openAI) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain_openAI) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain_openAI) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain_openAI) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain_openAI) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain_openAI) (2.32.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.104.2->langchain_openAI) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.104.2->langchain_openAI) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.104.2->langchain_openAI) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.104.2->langchain_openAI) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.76->langchain_openAI) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain_openAI) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain_openAI) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain_openAI) (0.24.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain_openAI) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain_openAI) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain_openAI) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openAI) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openAI) (2.5.0)\n",
            "Downloading langchain_openai-0.3.33-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.76-py3-none-any.whl (447 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-core, langchain_openAI\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.75\n",
            "    Uninstalling langchain-core-0.3.75:\n",
            "      Successfully uninstalled langchain-core-0.3.75\n",
            "Successfully installed langchain-core-0.3.76 langchain_openAI-0.3.33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9E3nDnfVD2Uc"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(\n",
        "    temperature=0.6,\n",
        "    model='glm-4.5',\n",
        "    openai_api_key=userdata.get('BOIYEN'),\n",
        "    openai_api_base=\"https://open.bigmodel.cn/api/paas/v4/\"\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "lwPZLoeHFaqR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke('what is llm').content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "collapsed": true,
        "id": "raIlWoI7HAHP",
        "outputId": "89b6cac9-9309-444d-8ff4-e417615bb2f6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'An **LLM** stands for **Large Language Model**. It\\'s a type of artificial intelligence (AI) designed to understand, generate, and interact with human language.\\n\\nHere\\'s a breakdown of what that means:\\n\\n1.  **Language Model (LM):**\\n    *   At its core, an LM is a computer program that learns patterns in human language.\\n    *   It predicts the next word (or sequence of words) in a sentence based on the context of the words that came before it.\\n    *   Think of it like a super-powered version of the autocomplete on your phone, but trained on vastly more data and capable of much more complex reasoning.\\n\\n2.  **Large (L):**\\n    *   This is the key differentiator. LLMs are \"large\" in two critical ways:\\n        *   **Massive Data:** They are trained on enormous datasets, often scraping a significant portion of the public internet, books, articles, code, and other text sources. This gives them a broad knowledge base.\\n        *   **Huge Model Size:** They have a massive number of internal parameters (often billions or even trillions). These parameters are like knobs the model adjusts during training to capture complex linguistic patterns, facts, reasoning abilities, and even some nuances of human communication.\\n\\n**What LLMs Can Do:**\\n\\nLLMs excel at tasks involving natural language:\\n\\n*   **Generate Text:** Write essays, poems, emails, code, marketing copy, stories, song lyrics, etc.\\n*   **Answer Questions:** Provide explanations, summaries, and factual information (though they can sometimes be wrong or \"hallucinate\").\\n*   **Translate Languages:** Convert text between different languages.\\n*   **Summarize Content:** Condense long documents or articles into key points.\\n*   **Converse:** Hold conversations, act as chatbots, or provide customer support.\\n*   **Analyze Sentiment:** Determine the emotional tone of text (positive, negative, neutral).\\n*   **Code Assistance:** Write, explain, debug, and translate code.\\n*   **Creative Brainstorming:** Help generate ideas for stories, products, marketing campaigns, etc.\\n\\n**How They Work (Simplified):**\\n\\n1.  **Training:** The LLM is fed its massive dataset. It processes this text, learning statistical relationships between words, phrases, concepts, and grammar rules. This process requires immense computational power (specialized hardware like GPUs/TPUs) and time.\\n2.  **Architecture:** Most modern LLMs are based on a deep learning architecture called the **Transformer**. This architecture is particularly good at handling long-range dependencies in text and understanding context effectively.\\n3.  **Inference (Using the Model):** Once trained, you give the LLM a prompt (a piece of text). The model uses its learned patterns and parameters to predict the most likely sequence of words to follow that prompt, generating a response.\\n\\n**Key Characteristics & Implications:**\\n\\n*   **Not Human, Not Conscious:** LLMs are sophisticated pattern-matching machines. They don\\'t \"understand\" language like humans do; they predict statistically probable outputs based on their training. They lack consciousness, beliefs, or true understanding.\\n*   **Knowledge Cutoff:** Their knowledge is frozen at the time of their last training update. They don\\'t inherently know about events that happened after that point unless specifically updated or connected to live data sources.\\n*   **Hallucinations:** Because they generate text based on patterns, they can sometimes confidently state incorrect information or facts that sound plausible but are made up. This is called \"hallucination.\"\\n*   **Bias:** LLMs can reflect and amplify biases present in their training data (e.g., societal biases related to gender, race, culture). Mitigating this is a major area of research.\\n*   **Versatility:** A single, well-trained LLM can perform a vast array of language tasks without needing specific task-specific training for each one (this is called \"few-shot\" or \"zero-shot\" learning).\\n*   **Foundation for AI Applications:** LLMs serve as powerful foundational models. They can be fine-tuned (further trained on specific data) for particular applications (e.g., medical diagnosis assistance, legal document analysis, customer service bots) or used as the \"brain\" within larger AI systems.\\n\\n**Examples of LLMs:**\\n\\n*   **GPT (Generative Pre-trained Transformer) series:** Developed by OpenAI (e.g., GPT-3, GPT-4, which power ChatGPT).\\n*   **LaMDA & PaLM:** Developed by Google (powering Bard and other Google AI features).\\n*   **Claude:** Developed by Anthropic.\\n*   **Llama:** Developed by Meta (Facebook).\\n*   **Mistral & Mixtral:** Developed by Mistral AI.\\n*   **Gemini:** Developed by Google (combining language and other modalities).\\n\\n**In short:** An LLM is a massive AI brain trained on huge amounts of text, capable of understanding and generating human-like language for a wide variety of tasks, powering many of the recent breakthroughs in conversational AI and content generation.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "-6F5VQ5nWrPe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PromptTemplate ='''talandila dzuwa is a student from takenolab,\n",
        "dzuwa he is a good humble boy,\n",
        "he always be consousenes during learning period,\n",
        "dzuwa is from lumbadzi on his stituation he was thing about ai classes,\n",
        "he mamanages to capture knowledge about ai.\n",
        "{context}\n",
        "{question}\n",
        "'''\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "b7IMXfCRHd4f"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "template=PromptTemplate.from_template(prompt)\n",
        "chain=template|model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "tHFyhpmxOVVc",
        "outputId": "b66f3e91-06e8-4ad0-8309-25b8f86fdffb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'prompt' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1558891868.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompts\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPromptTemplate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtemplate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPromptTemplate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m|\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'prompt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({'topic':'neural network'}).content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "oQ-svsXsPHTc",
        "outputId": "dc234d86-882a-42ff-de7d-092bb2b593e3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'chain' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3783753618.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'topic'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'neural network'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'chain' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template= promptTemplate.from_template(promptTemplate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "collapsed": true,
        "id": "yMdFbL69M6Pe",
        "outputId": "a574417a-ff64-4242-f0dd-dde9f1ded3d0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'promptTemplate' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3143480119.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtemplate\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpromptTemplate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpromptTemplate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'promptTemplate' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "chain = template|llm"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7Ic3tOm4JbTW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "7e6a8970-0ca6-4ede-a4c8-7567232cab82"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'template' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2447690656.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[0;34m|\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'template' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gxsUBAxBsMD9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke(' Neural networks, Quantum computing,Climate change')\n",
        "content(content),\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "collapsed": true,
        "id": "eUFo_uXtJyHX",
        "outputId": "4b15470e-a956-4e02-db41-b1fb361793a6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'chain' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4169713774.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' Neural networks, Quantum computing,Climate change'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'chain' is not defined"
          ]
        }
      ]
    }
  ]
}